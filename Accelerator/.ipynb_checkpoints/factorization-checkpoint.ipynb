{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def quantization(kernel):\n",
    "    r_max = np.max(kernel)\n",
    "    r_min = np.min(kernel)\n",
    "    quantized = np.int8(255 / (r_max - r_min) * kernel)\n",
    "    return quantized\n",
    "\n",
    "\n",
    "def convolve(input_data, conv_layer, bias, layer_num, padding=\"VALID\", stride=1):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_data: input data to the kernel\n",
    "    :param conv_layer: 4D convolution layer to be convolved into input_data\n",
    "    :param bias:\n",
    "    :param padding: if \"SAME\", we add padding to the input data so that\n",
    "                    the output would be the same size as input\n",
    "    :param stride:\n",
    "    :return: result of convolution\n",
    "    \"\"\"\n",
    "    if padding == \"SAME\":\n",
    "        p = int((conv_layer.shape[0] - 1) / 2)\n",
    "        input_data = np.pad(input_data, p, mode=\"constant\")\n",
    "\n",
    "    filter_num = conv_layer.shape[3]\n",
    "    output_width = int((input_data.shape[0] - conv_layer.shape[0]) / stride + 1)\n",
    "    output_size = [output_width, output_width, filter_num]\n",
    "    result = np.zeros(output_size)\n",
    "\n",
    "    conv_layer = quantization(conv_layer)\n",
    "\n",
    "    for f in range(filter_num):\n",
    "        kernel = conv_layer[:, :, :, f]\n",
    "        print('factoring')\n",
    "        repeated_w = conv_factorization(kernel)\n",
    "        print('convolving')\n",
    "        conv_result, weights_inputs_common = conv2d(input_data, kernel, repeated_w, stride)\n",
    "        result[:, :, f] = conv_result + bias[f]\n",
    "\n",
    "        # Experimental part\n",
    "        # -----------------------------\n",
    "        print(\"Filter\", f)\n",
    "        # common = get_common_regions(weights_inputs_common)\n",
    "        # write_common_regions_csv(common, 'layer-' + layer_num, f)\n",
    "\n",
    "        # print(\"Maximum repeated weight:\", max_index, \"Number of repeated:\", len(repeated_w[max_index][0]))\n",
    "        # print(\"Minimum repeated weight: \", min_index, \"Number of repeated:\", len(repeated_w[min_index][0]))\n",
    "\n",
    "        # print(\"Number of sum\", number_of_sum)\n",
    "        # print(\"Number of prod\", number_of_prod)\n",
    "\n",
    "        # -----------------------------------------\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_common_regions_csv(common, layer_name, filter_num):\n",
    "    import csv\n",
    "    csv.register_dialect('myDialect', delimiter=':', quoting=csv.QUOTE_ALL)\n",
    "    if filter_num == 0:\n",
    "        with open('common-' + layer_name + '.csv', 'w') as csvfile:\n",
    "            fieldnames = ['weights', 'positions']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=\"myDialect\")\n",
    "            writer.writeheader()\n",
    "            for i in common:\n",
    "                key = i + ('Filter' + str(filter_num + 1),)\n",
    "                writer.writerow({'weights': key, 'positions': common[i]})\n",
    "    else:\n",
    "        with open('common-' + layer_name + '.csv', 'a') as csvfile:\n",
    "            fieldnames = ['weights', 'positions']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            for i in common:\n",
    "                key = i + ('Filter' + str(filter_num + 1),)\n",
    "                writer.writerow({'weights': key, 'positions': common[i]})\n",
    "\n",
    "    print(\"writing completed\")\n",
    "\n",
    "\n",
    "def conv_factorization(kernel):\n",
    "    \"\"\"\n",
    "        It gets the kernel and returns a dictionary with shape(W, indexes) where W is the\n",
    "        weight and \"indexes\" is the indexes where W happens.\n",
    "    :param kernel:\n",
    "    :return:\n",
    "    repeated_dict: A dictionary of type: (key, value) where \"key\"s are the weights, and \"value\"s are\n",
    "    indexes of unique weight\n",
    "    max_index: The weight whose repeated indexes are most\n",
    "    min_index: The weight whose repeated indexes are least\n",
    "    \"\"\"\n",
    "    uniq = np.unique(kernel)\n",
    "    repeated_dict = {}\n",
    "    max = 0\n",
    "    min = np.inf\n",
    "    max_index = min_index = 0\n",
    "    for i in uniq:\n",
    "        index = np.where(kernel == i)\n",
    "\n",
    "        # Experimental part\n",
    "        if len(index[0]) > max:\n",
    "            max = len(index[0])\n",
    "            max_index = i\n",
    "        if len(index[0]) < min:\n",
    "            min = len(index[0])\n",
    "            min_index = i\n",
    "        # ----------------------------------\n",
    "\n",
    "        repeated_dict[i] = index\n",
    "    return repeated_dict  # , max_index, min_index\n",
    "\n",
    "\n",
    "def conv2d(data, kernel, repeated_position, stride=1):\n",
    "    \"\"\"\n",
    "        The kernel is 3d like in image with 3 channels RGB\n",
    "    :param data: is the actual input to convolution\n",
    "    :param kernel: is the kernel of convolution e.g. a 3 by 3 kernel\n",
    "    :param repeated_position: is the position of repeated weights\n",
    "    :param stride: steps of moving kernel\n",
    "    :return: the result of convolution\n",
    "    \"\"\"\n",
    "    result_size = int((data.shape[0] - kernel.shape[0]) / stride + 1)\n",
    "    result = np.zeros([result_size, result_size])\n",
    "    number_of_sum = np.zeros(4)\n",
    "    number_of_prod = np.zeros(4)\n",
    "    number_of_memory_access = np.zeros(4)\n",
    "    size_of_zero_weights = 0\n",
    "    weights_inputs_common = {}\n",
    "    for i in range(0, int((data.shape[0] - kernel.shape[0]) / stride) + 1, stride):\n",
    "        for j in range(0, int((data.shape[1] - kernel.shape[1]) / stride) + 1, stride):\n",
    "            temp_result = 0\n",
    "            for ind in repeated_position:\n",
    "                \"\"\" \n",
    "                    repeated_position[ind][0]: is the weight\n",
    "                    repeated_position[ind][1]: is the indexes\n",
    "                \"\"\"\n",
    "                if ind != 0:  # Zero weights\n",
    "                    temp_result += ind * np.sum(data[repeated_position[ind][0] + i,\n",
    "                                                     repeated_position[ind][1] + j,\n",
    "                                                     repeated_position[ind][2]])\n",
    "                    # Experimental part\n",
    "                    # mode4\n",
    "                    number_of_sum[3] += len(repeated_position[ind][0])\n",
    "                    number_of_prod[3] += 1\n",
    "                    number_of_memory_access[3] += (1  # reading weight\n",
    "                                                   +\n",
    "                                                   len(repeated_position[ind][\n",
    "                                                           0]))  # reading positions in input for that weight\n",
    "                    # -----------------------------------\n",
    "                else:\n",
    "                    size_of_zero_weights = len(repeated_position[ind][0])\n",
    "\n",
    "                # Experimental part\n",
    "                if i == j == 0:\n",
    "                    weights_inputs_common[ind] = np.empty([0, 3])\n",
    "\n",
    "                weights_inputs_common[ind] = np.append(weights_inputs_common[ind],\n",
    "                                                       np.concatenate(((repeated_position[ind][0] + i)[np.newaxis],\n",
    "                                                                       (repeated_position[ind][1] + j)[np.newaxis],\n",
    "                                                                       (repeated_position[ind][2])[np.newaxis]),\n",
    "                                                                      axis=0).T, axis=0)\n",
    "\n",
    "            # mode1\n",
    "            number_of_sum[0] += np.size(kernel)\n",
    "            number_of_prod[0] += np.size(kernel)\n",
    "            number_of_memory_access[0] += 2 * np.size(kernel)\n",
    "\n",
    "            # mode2\n",
    "            number_of_sum[1] += (np.size(kernel) - size_of_zero_weights)\n",
    "            number_of_prod[1] += (np.size(kernel) - size_of_zero_weights)\n",
    "\n",
    "            # we read zero weights only for understanding that they are zero.\n",
    "            # As soon as we understood, we ignore input data of that position\n",
    "            number_of_memory_access[1] += 2 * (np.size(kernel) - size_of_zero_weights) + size_of_zero_weights\n",
    "\n",
    "            # mode3\n",
    "            # number_of_sum[3] +=\n",
    "            # -----------------------------------\n",
    "\n",
    "            result[i, j] = temp_result\n",
    "    return result, weights_inputs_common  # , number_of_sum, number_of_prod\n",
    "\n",
    "\n",
    "def get_common_regions(weights_inputs_common):\n",
    "    common = {}\n",
    "    for i in weights_inputs_common:\n",
    "        for j in weights_inputs_common:\n",
    "            if i != j and ((j, i) not in common.keys()):\n",
    "                common[(i, j)] = intersect_along_first_axis(weights_inputs_common[i], weights_inputs_common[j])\n",
    "    return common\n",
    "\n",
    "\n",
    "def intersect_along_first_axis(a, b):\n",
    "    # check that casting to void will create equal size elements\n",
    "    assert a.shape[1:] == b.shape[1:]\n",
    "    assert a.dtype == b.dtype\n",
    "\n",
    "    # compute dtypes\n",
    "    void_dt = np.dtype((np.void, a.dtype.itemsize * np.prod(a.shape[1:])))\n",
    "    orig_dt = np.dtype((a.dtype, a.shape[1:]))\n",
    "\n",
    "    # convert to 1d void arrays\n",
    "    a = np.ascontiguousarray(a)\n",
    "    b = np.ascontiguousarray(b)\n",
    "    a_void = a.reshape(a.shape[0], -1).view(void_dt)\n",
    "    b_void = b.reshape(b.shape[0], -1).view(void_dt)\n",
    "\n",
    "    # intersect, then convert back\n",
    "    return np.intersect1d(b_void, a_void).view(orig_dt)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
